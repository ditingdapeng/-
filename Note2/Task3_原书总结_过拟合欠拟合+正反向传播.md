# 模型选择&欠拟合&过拟合

## 训练误差和泛化误差

训练误差指的是在训练数据集上表现出的误差，泛化误差常通过测试数据上的误差来近似，指的是模型在任意一个测试数据样本上表现出的误差的期望。常通过损失函数来计算训练误差和泛化误差。

## 模型选择

通过验证数据集的方式来选择模型：预留一部分在训练数据集和测试数据集以外的数据来进行模型选择，这部分数据称为验证数据集。选取验证数据集的方式一般为K折交叉验证：

1. 把原始训练数据集分割成K个不重合的子数据集，然后做K次模型训练和验证；
2. 每一次使用1个子数据集验证模型，并使用其他K-1个子数据集来训练模型；
3. 最后对这K次训练误差和验证误差分别求平均；

## 欠拟合和过拟合

欠拟合：模型无法得到较低的训练误差；

过拟合：模型的训练误差远小于它在测试数据集上的误差；

导致欠拟合和过拟合主要有两个因素：模型的复杂度和训练数据集的大小。相对应的，应对过拟合的方法有四种：调整模型的复杂度，增大训练数据集，权重衰减，以及使用丢弃法。

## 权重衰减

权重衰减等价于L2范数正则化，正则化通过在模型损失函数基础上添加L2范数惩罚项，使得学出的模型参数值变小，从而得到训练所需的最小化的函数，来应对过拟合的问题。

L2范数惩罚项：模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归的损失函数为例：添加惩罚项后的损失函数为$l(w_1,w_2,b)+\frac{\lambda}{2n}||w||^2$

当$\lambda$比较大时，惩罚项在损失函数中的比重就比较大，为了让损失函数l更小，那么通常会使得学到的权重系数w更接近于0；

对于梯度下降中权重w的修改方式的变化：L2范数正则令权重w1和w2先自乘小于1的数，再减去不含惩罚项的梯度。通过惩罚绝对值较大的模型参数，为需要学习的模型增加了限制，使得对过拟合产生效果。

### 手动实现

以线性回归为例：

1. 初始化模型参数：随机初始化模型参数w,b，并为每个参数都附上梯度；
2. 定义L2范数惩罚项，这里只惩罚模型的权重系数：(w**2)/sum()/2；
3. 定义训练和测试过程，在损失函数后添加L2范数的惩罚项；
4. 观察改变lambda参数后，改变过拟合的情况

###  简洁实现

可以通过Gluon的wd超参数来指定lambda的值，定义多个Trainer实例来对不同的模型参数使用不同的迭代方法->

对于线性回归而言：需要分别对权重和偏差构造Trainer实例，从而完成权重参数的衰减，而不对偏差参数衰减。(默认是对权重和偏差同时衰减)：

1. 先对Sequential实例化，定义网络层为net：net = nn.Sequential()

2. 在net中添加一个全连接层：net.add(nn.Dense(1))

3. 对参数w和b进行初始化：net.initialize(init.Normal(sigma=1))

4. 对权重参数进行L2正则化：gluon.Trainer(net.collect_params('.*weight'), 'sgd', {'learning_rate':lr, 'wd':wd})

   不对偏差系数进行L2正则化：gluon.Trainer(net.collect_params('.*bias'), 'sgd', {'learning_rate': lr})

5. 在每一个batch_size的循环中，对trainer_w和trainer_b分别调用step函数，从而分别更新权重和偏差；

**（3.12 练习题未做）**

## 倒置丢弃法
倒置丢弃法是针对隐藏层来说的，表达隐藏层的单元将有一定概率被丢弃掉。由于训练时隐藏层神经元的丢弃是随机的，所以可以让输出层的计算无法过度依赖隐藏层神经元种的任何一个，从而起到正则化的作用，用来应对过拟合。
设丢弃概率为p，那么有p的概率h会清零，有1-p的概率将h除以1-p做标准化拉伸。
为什么要除以1-p，是因为除以1-p后，丢弃法就可以保证原期望值不变。
### 手动实现
1. 首先定义dropout函数，表示以prob的概率丢弃ndarray输入X中的元素；
	如果传入的概率为1的话，就全部丢弃，返回全转化为0的值：return X.zeros_like()
	否则就从(0,1)之间随机生成一个正态分布的值，跟保留的概率做比较：
	return (nd.random.uniform(0, 1, X.shape) < (1 - drop_prob)) * X / (1 - drop_prob)
2. 定义模型参数：同样定义好W和b的值，根据隐藏层和输出层矩阵的大小来定义W;
3. 定义模型：对隐藏层使用过激活函数后再使用丢弃法，通常把靠近输入层的丢弃概率设的小一点，仅在训练模式下使用丢弃法；
4. 训练和测试模型：和之前多层感知机类似；

### 简洁实现
在全连接层后添加Dropout层并指定丢失概率：
训练模型时，Dropout层以指定的丢弃概率随机丢弃上一层的输出元素；
测试模型时，Dropout层不发挥作用。
1. 对Sequential进行初始化为net层；
2. 在net层中添加激活函数和dropout层；
3. 通过Trainer类来训练模型；

**（3.13 习题未完成）**

​	

# 数值稳定性&模型初始化

影响数值稳定性的典型问题就是衰减和爆炸：

当神经网络的层数较多时，衰减就是当权重系数小于1时，经过多个层后，梯度经过类乘后会变得极小；梯度爆炸就是当权重较大后，经过每层的累乘后乘积会变得极大。

需要随机初始化模型权重参数的原因：如果隐藏层的权重系数都是相等的值，那么无论隐藏层有多少，本质上只有一个隐藏单元在发挥作用。

1. 正向传播中：如果将每个隐藏单元的参数都为相等的值，那么在正向传播时每个隐藏单元将根据相同的输入计算出相同的值，并传递至输出层；
2. 反向传播中：每个隐藏单元的参数梯度值相同，这些参数在梯度迭代后仍然相等。这时无论隐藏层有多少，本质上只有一个隐藏单元在发挥作用。

常常采用net.initialize(init.Normal(sigma=0.01))使得模型net的权重系数采用正态分布的随机初始化方式。

**（3.15刚刚的初始化只是说的隐藏层，能否把线性回归或softmax回归的权重系数初始化为相同值）**

# 实战：房价预测








