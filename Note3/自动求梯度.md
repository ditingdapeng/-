# 自动求梯度

通过```from mxnet import autograd,nd```模块来自动求梯度。

当对一个表达式中的变量求梯度时：

1. 首先需要创建变量，并赋予其初值：

​       x = nd.arange(4).reshape((4, 1))

2. 调用attach_grad函数来申请存储梯度所需要的内存：

​       x.attach_grad()

3. 默认条件下MXNet不会记录用于求梯度的计算，需要调用recore函数来让MXNet记录与求梯度有关的计算：

   with autograd.record():

   ​	y = 2 * nd.dot(x.T, x)

   这里autograd.record()下，需要写的是表达式的函数，这里表达式的函数是：$y = 2*x^2$

   record函数的作用：1. 记录并计算梯度；2. 将运行模型从预测模式转为训练模式；(有些情况下,同一个模型在训练模式和预测模式下的行为并不相同)

4. 如果得到的y是一个数的话，就调用backward来自动求梯度；如果得到的y不是一个数，那么MXNet将默认先对y中元素求和得到新的变量，再求该变量有关的x的梯度；

即使函数的计算图中包含了Python的条件和循环控制，也有可能对变量求梯度：同样使用record函数记录计算，并调用backward函数求梯度。



# 正向传播&反向传播&计算图

之前模型求梯度的过程：是对输入计算模型输出，然后通过autograd模块来调用系统自动生成的backward函数计算梯度。下面将具体来解释正向传播和反向传播的实现过程：

## 正向传播

正向传播是指对神经网络沿着从输入层到输出层的顺序，依次计算并存储模型的中间变量(包括输出)。

以带L2范数正则化的单隐藏层的多层感知机为例子：输入层->隐藏层->输出层

1. 输入为x的样本，第一个隐藏层的权重系数为$W^{(1)}$，得到的中间变量$z = W^{(1)}x$;
2. 对隐藏层的输出进行激活函数的激活：$h = \phi(z)$;
3. 激活后的h为隐藏层的输出，当输出层的权重为$W^{(2)}$时，可以得到输出层的输出为：$o = W^{(2)}h$
4. 由输出层的输出和样本标签y，可以由损失函数L计算出单个数据样本的损失L:$L = l(o, y)$
5. 对于隐藏层的权重$W^{(1)}$和输出层的权重$W^{(2)}$，添加正则化项：$s = \frac{\lambda}{2}(||W^{(1)}||^2+||W^{(2)}||^2)$
6. 将原来单个数据样本的损失加上正则项：$J = L + s$

## 反向传播

反向传播是计算神经网络参数梯度的方法，依照微积分中的链式法则，沿着从输出层到输入层的顺序，依次计算并存储目标函数有关神经网络各层的中间变量以及参数的梯度。

(反向传播得过程待补充)









