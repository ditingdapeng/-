# 卷积神经网络

卷积神经网络时含有卷积层的神经网络。下面将描述卷积神经网络中卷积层和池化层的工作原理，并解释填充、步幅、输入通道和输出通道的含义。

## 二维卷积层

有高和宽两个维度，常用来处理图像数据。

### 二维互相关运算

在二维卷积层中，一个二维输入数组和一个二维核数组通过互相关运算输出一个二维数组。

例：输入为$3*3$，核数组为$2*2$，其中核数组在计算中又叫做卷积核，得到的输出数组的计算方式是：卷积窗口从输入数组的左上方开始，按照卷积窗口的大小从左到右、从上往下的顺序，依次在输入数组上滑动，当滑动到一个位置时，对应数组进行点乘运算，得到的值作为输出数组位置上的值。

自定义corr2d函数进行实现(**未详细解释函数**)



### 二维卷积层

二维卷积层将输入和卷积核做互相关运算，并加上一个标量偏差来得到输出。

训练模型时，通常先对卷积核进行随机初始化，然后不断迭代模型参数（卷积核和标量偏差）。

### 二维卷积层的应用--边缘检测

卷积层可通过重复使用卷积核来有效的表征局部空间。

输入数组为中间为黑，其余为白的图像，通过构造高和宽分别为1和2的卷积核1和-1，来对输入做互相关运算，最终将从白到黑的边缘和从黑到白的边缘分别检测为1和-1。

### 核数组

核数组通过输入数据X和输出数据Y来定义。以边缘检测为例(卷积层忽略了偏差)：

1. 首先构造一个卷积层，将其卷积核初始化为随机数组；
2. 在每次迭代中，使用平方误差来比较输出数据Y和卷积层的输出；
3. 计算梯度来更新权重；

### 卷积运算

将核数组左右翻转并上下翻转，再与输入数组做互相关运算。

因为可以把核数组进行改变使得卷积运算和互相关运算的结果相同，故卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。

### 特征图和感受野

特征图：二维卷积层输出的二维数组，可以 看作是由输入在空间维度某一级的表征；

单元x的感受野：单元x的前向网络中的输入区域影响神经网络中单元x的单元；

(**5.1 习题未作**)

## 填充和步幅

卷积层的输出形状由输入形状和卷积核窗口决定，输出形状为：(输入高 - 卷积核高+1)*(输入宽 - 卷积核宽 + 1)。

通过调整卷积层中的两个超参数填充和步幅，来改变卷积层的输出形状。

###  填充

指在输入高和宽的两侧填充元素。

### 步幅

将每次滑动的行数和列数称为步幅；

## 多输入通道和多输出通道

### 多输入通道

### 多输出通道

### 1*1 卷积层



## 池化层

为了缓解卷积层对位置的过度敏感性。



## LeNet 模型
LeNet分为卷积层块和全连接层块。
1. 卷积层块：卷积层+最大池化层
	卷积层用来识别图像里的空间模式，如线条和局部；每个卷积层都使用$5*5$的窗口，并在输出上使用sigmoid激活函数。
	第二个卷积层比第一个卷积层的输入的高和宽要小，故增加输出通道使这两个卷积层的参数尺寸类似。
	最大池化层用来降低卷积层对位置的敏感性；
2. 全连接层块：
	会将小批量中每个样本变平，全连接层的输入为二维，第一维是小批量中的样本，第二维是每个样本变平后的向量，向量长度为通道，高和宽的乘积。

## 深度卷积神经网络(AlexNet)
为了表征足够复杂的输入，可以用多层神经网络来分级的表示特征，并逐级表示越来越抽象的概念或模式。
AlexNet在LeNet的基础上增加了3个卷积层，分为卷积+全连接隐藏层+全连接输出层；
将simoid改成了更加简单的Relu激活函数：

1. Relu激活函数的计算更简单，其没有像sigmoid函数中的求幂运算；
2. ReLu激活函数在不同的参数初始化方法下使得模型更容易的训练；(sigmoid容易产生梯度消失问题--sigmoid函数输出值接近0或1时，这些区域的梯度几乎为0，从而造成反向传播无法继续更新部分模型参数，但是Relu激活函数在正区间的梯度恒为1)
3. AlexNet使用丢弃法来控制全连接层的模型复杂度，而LeNet并没有使用丢弃法；
4. AlexNet进一步扩大了不同特征的数据集来缓解过拟合；

**（代码阅读，对层数设定的值应该是多少来进行判断和学习）**
**问题：和LeNet的结果相比，AlexNet的结果有什么区别？**

## 使用重复元素的网络(VGG)
通过重复使用简单的基础块来构建深度模型。
连续使用多个相同的填充为1、窗口形状为$3*3$的卷积层后接上一个步幅为2、窗口形状为$2*2$的最大池化层。
卷积层保持输入的高和宽不变，而池化层减半；
vgg由卷积层+全连接组成：

1. 定义卷积层模块vgg_block：卷积层+最大池化层；
2. 在vgg中，串联多个vgg_block，指定每个vgg块里卷积层个数和输出通道数；
3. 
VGG网络由卷积层模块后接全连接层模块构成：卷积层模块使用vgg_block

## NiN网络中的网络
串联网络
其他网络中的全连接层都是放在卷积的后面，NiN中将$1*1$的卷积层代替全连接层，从而使空间信息能够自然传递到后面的层中去。其高和宽上的每个元素相当于样本，而通道相当于特征。

## 含并行连结的网络GoogLeNet
主体使用了5个模块；



# 批量归一化

对输入数据进行标准化处理对浅层模型是有效的，仍需要用批量归一化的方法应对深层模型。

1. 全连接层的批量归一化：将批量归一化层置于全连接层中的仿射变化和激活函数之间；
2. 卷积层的批量归一化：在卷积计算之后，应用激活函数之前。如果卷积计算输出多个通道，需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数；

为了保证让单个样本的输出不取决于批量归一化所需要的随机小批量中的均值和方差，通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。

**(代码还未详细看)**

## 手动实现

## 简洁实现

**5.10 习题未作**

# 残差网络
## 残差块
将加权运算的权重和偏差参数学成0，拟合出有关恒等映射的残差映射f(x)-x；
残差块中首先有两个有相同输出通道数的$3*3$的卷积层，每个卷积层后接一个批量归一化层和ReLU激活函数。

## ResNet模型
ResNet的前两层和GoogleNet一样，在卷积层后接入最大池化层。只是比GoogleNet多了在卷积层后增加的批量归一化层。

# 稠密连接网络(DenseNet)
跨层连接设计；


