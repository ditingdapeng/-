# 线性回归

线性回归神经网络适合于输出为连续值得情景。

## 1. 损失函数

在模型训练中，需要衡量预测值和真实值之间得误差，那么通常是选取一个非负数作为误差，且数值越小表示误差越小。

当使得误差最小化的损失函数问题的解可以直接用公式来表达出来时，这类问题叫做解析解。如果不存在解析解的话，可以采用数值解的方法：通过优化算法有限次迭代模型参数来尽可能的降低损失函数的值。

**数值解优化算法**：小批量随机梯度下降->

1. 先随机选取一组模型参数的初始值，接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值；
2. 在每次迭代中，先随机均匀采样一个由固定树木训练数据样本所组成的小批量(mini-batch)，然后求小批量中数据样本的平均损失有关模型参数的导数(梯度)；
3. 用此梯度结果和预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。

超参数： 不是通过模型训练学出的，而是人为设定的参数(如：这里的批量大小和学习率)；

## 2. 线性回归神经网络

**全连接层**：输出层中的神经元和输入层中各个输入完全连接，这样的输出层叫做全连接层，或者稠密层。

### 手动实现

线性回归神经网络的基本元素包括：模型、训练数据、损失函数和优化算法。

**2.1 生成数据集**：生成数据集的过程中，除了xw+b生成的标签值外，再加上服从均值为0，标准差为0.01的正态分布。

**2.2 读取数据**：训练模型时，需要遍历数据集并不断读取小批量的数据样本；

**2.3 初始化模型参数**：将权重初始化为均值为0，标准差为0.01的符合正态分布的随机数，偏差b则初始化为0，并对创建的w和b这两个参数创建它们的梯度，目的是需要对这些参数求梯度来迭代参数的值；

**2.4 定义模型**：定义线性回归wx+b的向量化函数；

**2.5 定义损失函数**：使用平方损失来定义线性回归的损失函数，返回的结果要和y_hat的形式相同，真实的也要和y_hat的形式相同。

**2.6 定义优化算法**：定义自动求梯度的模块，这个模块是用来求一个批量样本的梯度和，用这个梯度和除以批量的大小得到这个param的平均值；

**2.7 训练模型**：在每次迭代求模型参数时，根据当前读取的一个batch_size的数据样本，通过调用反向传播函数backward来计算小批量随机梯度，并调用优化算法随机梯度下降，来迭代求出模型参数。

**(3.2 书中问题未解答)**

### 简洁实现

**2.1 生成数据集**：和手动实现道理一样；

**2.2 读取数据集**：通过DataLoader的函数进行随机读取小批量；

**2.3 定义模型**：通过导入nn模块，先定义一个Sequential实例的模型变量net。Sequential实例可以看作是一个串联各个层的容器，在构造模型时，要在这个容器中依次添加层，处理输入的数据并传给下一层；

全连接层是一个Dense实例，通过nn.Dense来进行创建；

**2.4 初始化模型参数**：使用net前，需要通过init来初始化模型的参数，通过init.Normal的方法来生成符合正态分布的权重和零值的偏差；

**2.5 定义损失函数**：使用loss模块机械能损失函数的导入；

**2.7 定义优化方法**：创建一个Trainer实例，该实例会指定优化算法，并使用优化算法来迭代net实例中add嵌套的所有层以及参数；

**2.7 训练模型**：通过Trainer实例的step函数来迭代模型参数，通过对损失函数的backward()，来求梯度下降后的损失函数的值；

**(3.3 书中问题未解答)**



# Softmax回归

SoftMax回归适合于模型输出为离散值的情景。

## 1. 分类问题的损失函数

要选择使用更加适合离散值输出的模型来解决分类问题，考虑使用softmax回归模型。

**softmax回归模型**：和线性回归的相同点：都是将输入特征与权重做线性叠加；都是单层的神经网络，输出层是全连接层；和线性回归的不同点：softmax回归的输出值个数等于标签里的类别数。

**softmax运算**：为了得到离散值的输出，可以将值最大的输出所对应的类作为预测输出，但是这样直接使用输出层的问题在于：输出值得范围不确定，很难来衡量离散值和不确定范围得输出值之间得误差。

于是可以考虑softmax运算符，其先将输出值变换为值为正且和为1的概率分布，softmax运算不改变预测类别的输出。(**softmax运算是个撒**)

**分类的矢量计算表达式**：单样本的话就是softmax(xW + b)，这里的x指的是行向量x；对于小批量的话，其加法运算就使用了广播机制的方法，最终输出样本的概率分布。

**交叉熵损失函数**：softmax运算将输出变换为一个合法的类别预测分布，真实的样本标签也可以用类别分布来表达。这里的类别分布指的就是每个样本的标签值是一个向量，是哪个类这个向量对应位置上的值就为1，否则为0。训练目标是使预测概率分布y_hat尽可能的接近真实的标签概率分布y。

那么用什么损失函数可以更好的来衡量预测概率分布和真实概率分布是否接近呢？因为只需要去衡量两个概率分布的差异，而并不需要预测概率完全等于标签概率，平方损失函数过于严格。这里可以考虑交叉熵函数：交叉熵只关心对正确类别的预测概率，最小化交叉熵损失函数等价于最大化训练数据集中所有标签类别的联合预测概率。

**(3.4习题未作)**

## 2. Fashion-Mnist图像分类数据集

### 获取和读取数据集

1. 通过Gluon的data包来下载数据，参数train为True的话是训练集，为False的话是测试集；

2. 将类别的数值标签转化为相应的文本标签；
3. 通过直接创建DataLoader实例，每次读取一个样本数为batch_size的小批量数据；

**(3.5习题未作)**

## 3. Softmax回归神经网络

### 手动实现

步骤：1. 获取并读取数据；2. 定义模型和损失函数；3. 使用优化算法训练模型；

**3.1 获取和读取数据** 

设置参数为batch_size，读取batch_size的数据大小作为train和test；

**3.2 初始化模型参数** 

将权重初始化为均值为0，标准差为0.01的符合正态分布的随机数，偏差b则初始化为0，并对创建的w和b这两个参数创建它们的梯度，目的是需要对这些参数求梯度来迭代参数的值；

**3.3 实现softmax运算**

对多维ndarray按维度操作：

对同一列进行求和，并在结果中保留行和列这两个维度：X.sum(axis=0, keepdims=True)；

对同一行进行求和，并在结果中保留行和列这两个维度：X.sum(axis=1, keepdims=True)

softmax运算是为了表达出样本预测各个输出的概率，得到的矩阵每行元素和为1且非负，输出矩阵中的任意一行元素表示该样本在各个输出类别上的预测概率：

一. 先通过exp函数对每个元素做指数运算;

二. 再对exp矩阵的同行元素求和；

三. 最后令矩阵每行各元素与该行元素之和相除。

**3.4 定义模型**

将输入的图像通过reshape改为长度为num_inputs的向量，再通过softmax函数进行计算；

**3.5 定义交叉熵损失函数**

通过pick函数来实现交叉熵损失函数模块：-nd.pick(y_hat, y).log()

**3.6 计算分类准确率**

得到的y_hat是一个向量，即预测结果的概率分布，把预测概率最大的类别当作是输出类别。分类准确率为正确预测数量(输出类别与真实类别一致)与总预测数量之比，

**3.7 训练模型**

和线性回归类似

**（3.6节习题未作）**

### 简洁实现

**3.1 获取和读取数据**

和手动实现一样，通过load_data_fashion_mnist函数，通过传入参数batch_size来加载一批量的数据；

**3.2 定义和初始化模型**

通过初始化nn.Sequential，创建一个net，向net中添加全连接层，并通过net.initialize的方法来初始化模型的权重系数；

**3.3 交叉熵损失函数**

分开定义softmax运算和交叉熵损失可能会造成数值上的不稳定，可以使用Gluon提供的softmax函数；

**3.4 定义优化方法**

通过Gluon的Trainer方法，传入定义的net的param参数，以及sgd的学习率进行优化，返回定义的变量trainer；

**3.5 训练模型**

训练方式照旧；



# 多层感知机

线性回归和softmax回归均是处理回归和分类问题的单层神经网络，输出层中的神经元和输入层中各个输入完全连接。而多层感知机，就是在前两者的基础上，增加了**隐藏层**以及引入了**非线性变换的激活函数**。

## 1. 隐藏层和激活函数

**隐藏层**：多层感知机在单层神经网络的基础上引入了一到多个位于输入层和输出层之间的隐藏层，隐藏层和输出层都是全连接层。如果仅引入隐藏层的话，联立后的式子与仅含有输出层的单层神经网络等价。

**激活函数**：等价的原因在于全连接层只是对数据做仿射变换，解决问题的方法是引入非线性变换。将非线性函数处理过后的结果作为下一层的输入，这个非线性函数叫做激活函数。

**常用的激活函数有**：ReLU，sigmoid，tanh；

**ReLU**：

1. 函数：max(x,0)，图像为一个两段的线性函数‘
2. 性质：只保留正数元素，并将负数元素清零；
3. 导数：输入为负数时，ReLU函数的导数为0，当输入为正数时，ReLu函数的导数为1

**Sigmoid**：

1. 函数：$\frac{1}{1+exp(-x)}$
2. 性质：将值域控制到0和1之间，函数为S型的曲线；
3. 导数：sigmoid(x)(1 - sigmoid(x))，当输入为0时，导数达到最大值0.25；当输入偏离0时，导数越接近于0；

**Tanh**：

1. 函数：$\frac{1-exp(-2x)}{1+exp(-2x)}$
2. 性质：将值变换到-1和1之间，函数也为S形曲线；
3. 导数：$1-tanh^2(x)$，类似正态分布的曲线：当输入为0时，导数达到最大值1，当输入越偏离0时，导数越接近0.

## 2. 多层感知机神经网络

### 手动实现

**2.1 获取和读取数据**

通过d2l的load_data_fashion_mnist函数，传入batch_size参数来获取train和test的数据；

**2.2 定义模型参数**

设定一个隐藏层，一个输入层，一个输出层。输入的为图像形状相乘的像素，输出为类别数，隐藏层的单元数是超参数需要人为设定。W1和W2分别通过nd.random.normal的方法来进行随机初始化。

**2.3 定义激活函数**

使用基础的nd.maxiumu来实现Relu

**2.4 定义神经网络模型**

通过reshape函数将每张原始图像改为长度为num_inputs的向量：

隐藏层->nd.dot(X, W1) +b1后，再经过relu激活函数激活，输出结果传入输出层，当作输出层的输入；

输出层->将relu激活后的值记为H，再将H送入全连接层中，得：nd.dot(H, W2) + b2

**2.5 定义损失函数**

使用Gluon得SoftmaxCrossEntropyLoss来计算softmax和交叉熵损失；

**2.6 训练模型**

训练模型的过程和之前没有区别；

## 简洁实现

**2.1 定义模型**

首先实例化nn.Sequential，定义变量为net；接着向net中添加激活函数为relu函数的隐藏层，以及长度为类别个数的输出层；初始化Sequential实例化后的权重，参数为正态分布。

**2.2 读取数据并训练模型**

首先通过load_data_fashion_mnist来读取一个batch_size的数据；

接着通过gloss.SoftmaxCrossEntropyLoss()来计算softmax和交叉熵的损失；

通过向gluon.Trainer函数中传入之前定义的模型的参数，以及指定sgd为优化方法，定义好训练模型；

对每一批次的训练模型进行迭代训练；

















































































