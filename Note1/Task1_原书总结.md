# 线性回归

线性回归神经网络适合于输出为连续值得情景。

## 1. 损失函数

在模型训练中，需要衡量预测值和真实值之间得误差，那么通常是选取一个非负数作为误差，且数值越小表示误差越小。

当使得误差最小化的损失函数问题的解可以直接用公式来表达出来时，这类问题叫做解析解。如果不存在解析解的话，可以采用数值解的方法：通过优化算法有限次迭代模型参数来尽可能的降低损失函数的值。

**数值解优化算法**：小批量随机梯度下降->

1. 先随机选取一组模型参数的初始值，接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值；
2. 在每次迭代中，先随机均匀采样一个由固定树木训练数据样本所组成的小批量(mini-batch)，然后求小批量中数据样本的平均损失有关模型参数的导数(梯度)；
3. 用此梯度结果和预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。

超参数： 不是通过模型训练学出的，而是人为设定的参数(如：这里的批量大小和学习率)；

## 2. 线性回归神经网络

**全连接层**：输出层中的神经元和输入层中各个输入完全连接，这样的输出层叫做全连接层，或者稠密层。

### 手动实现

线性回归神经网络的基本元素包括：模型、训练数据、损失函数和优化算法。

**2.1 生成数据集**：生成数据集的过程中，除了xw+b生成的标签值外，再加上服从均值为0，标准差为0.01的正态分布。

**2.2 读取数据**：训练模型时，需要遍历数据集并不断读取小批量的数据样本；

**2.3 初始化模型参数**：将权重初始化为均值为0，标准差为0.01的符合正态分布的随机数，偏差b则初始化为0，并对创建的w和b这两个参数创建它们的梯度，目的是需要对这些参数求梯度来迭代参数的值；

**2.4 定义模型：**定义线性回归wx+b的向量化函数；

**2.5 定义损失函数：**使用平方损失来定义线性回归的损失函数，返回的结果要和y_hat的形式相同，真实的也要和y_hat的形式相同。

**2.6 定义优化算法：**定义自动求梯度的模块，这个模块是用来求一个批量样本的梯度和，用这个梯度和除以批量的大小得到这个param的平均值；

**2.7 训练模型：**在每次迭代求模型参数时，根据当前读取的一个batch_size的数据样本，通过调用反向传播函数backward来计算小批量随机梯度，并调用优化算法随机梯度下降，来迭代求出模型参数。

**(3.2 书中问题未解答)**

### 简洁实现

**2.1 生成数据集 ：**和手动实现道理一样；

**2.2 读取数据集：**：通过DataLoader的函数进行随机读取小批量；

**2.3 定义模型：**通过导入nn模块，先定义一个Sequential实例的模型变量net。Sequential实例可以看作是一个串联各个层的容器，在构造模型时，要在这个容器中依次添加层，处理输入的数据并传给下一层；

全连接层是一个Dense实例，通过nn.Dense来进行创建；

**2.4 初始化模型参数：**使用net前，需要通过init来初始化模型的参数，通过init.Normal的方法来生成符合正态分布的权重和零值的偏差；

**2.5 定义损失函数：**使用loss模块机械能损失函数的导入；

**2.7 定义优化方法：**创建一个Trainer实例，该实例会指定优化算法，并使用优化算法来迭代net实例中add嵌套的所有层以及参数；

**2.7 训练模型：**通过Trainer实例的step函数来迭代模型参数，通过对损失函数的backward()，来求梯度下降后的损失函数的值；

**(3.3 书中问题未解答)**



# Softmax回归

SoftMax回归适合于模型输出为离散值的情景。

## 1. 分类问题的损失函数

要选择使用更加适合离散值输出的模型来解决分类问题，考虑使用softmax回归模型。

**softmax回归模型：**和线性回归的相同点：都是将输入特征与权重做线性叠加；都是单层的神经网络，输出层是全连接层；和线性回归的不同点：softmax回归的输出值个数等于标签里的类别数。

**softmax运算：**为了得到离散值的输出，可以将值最大的输出所对应的类作为预测输出，但是这样直接使用输出层的问题在于：输出值得范围不确定，很难来衡量离散值和不确定范围得输出值之间得误差。

于是可以考虑softmax运算符，其先将输出值变换为值为正且和为1的概率分布，softmax运算不改变预测类别的输出。(**softmax运算是个撒**)

**分类的矢量计算表达式：**单样本的话就是softmax(xW + b)，这里的x指的是行向量x；对于小批量的话，其加法运算就使用了广播机制的方法，最终输出样本的概率分布。

**交叉熵损失函数：**softmax运算将输出变换为一个合法的类别预测分布，真实的样本标签也可以用类别分布来表达。这里的类别分布指的就是每个样本的标签值是一个向量，是哪个类这个向量对应位置上的值就为1，否则为0。训练目标是使预测概率分布y_hat尽可能的接近真实的标签概率分布y。

那么用什么损失函数可以更好的来衡量预测概率分布和真实概率分布是否接近呢？因为只需要去衡量两个概率分布的差异，而并不需要预测概率完全等于标签概率，平方损失函数过于严格。这里可以考虑交叉熵函数：交叉熵只关心对正确类别的预测概率，最小化交叉熵损失函数等价于最大化训练数据集中所有标签类别的联合预测概率。

**(3.4习题未作)**

## 2. Fashion-Mnist图像分类数据集

### 获取和读取数据集

1. 通过Gluon的data包来下载数据，参数train为True的话是训练集，为False的话是测试集；

2. 将类别的数值标签转化为相应的文本标签；
3. 通过直接创建DataLoader实例，每次读取一个样本数为batch_size的小批量数据；

**(3.5习题未作)**

## 3. Softmax回归神经网络

### 手动实现

步骤：1. 获取并读取数据；2. 定义模型和损失函数；3. 使用优化算法训练模型；

**3.1 获取和读取数据** 

设置参数为batch_size，读取batch_size的数据大小作为train和test；

**3.2 初始化模型参数** 

将权重初始化为均值为0，标准差为0.01的符合正态分布的随机数，偏差b则初始化为0，并对创建的w和b这两个参数创建它们的梯度，目的是需要对这些参数求梯度来迭代参数的值；

**3.3 实现softmax运算**

对多维ndarray按维度操作：

对同一列进行求和，并在结果中保留行和列这两个维度：X.sum(axis=0, keepdims=True)；

对同一行进行求和，并在结果中保留行和列这两个维度：X.sum(axis=1, keepdims=True)

softmax运算是为了表达出样本预测各个输出的概率，得到的矩阵每行元素和为1且非负，输出矩阵中的任意一行元素表示该样本在各个输出类别上的预测概率：

一. 先通过exp函数对每个元素做指数运算;

二. 再对exp矩阵的同行元素求和；

三. 最后令矩阵每行各元素与该行元素之和相除。

**3.4 定义模型**

将输入的图像通过reshape改为长度为num_inputs的向量，再通过softmax函数进行计算；

**3.5 定义交叉熵损失函数**

通过pick函数来实现交叉熵损失函数模块：-nd.pick(y_hat, y).log()

**3.6 计算分类准确率**

得到的y_hat是一个向量，即预测结果的概率分布，把预测概率最大的类别当作是输出类别。分类准确率为正确预测数量(输出类别与真实类别一致)与总预测数量之比，

**3.7 训练模型**

和线性回归类似

**（3.6节习题未作）**

### 简洁实现

**3.1 获取和读取数据**

和手动实现一样，通过load_data_fashion_mnist函数，通过传入参数batch_size来加载一批量的数据；

**3.2 定义和初始化模型**

通过初始化nn.Sequential，创建一个net，向net中添加全连接层，并通过net.initialize的方法来初始化模型的权重系数；

**3.3 交叉熵损失函数**

分开定义softmax运算和交叉熵损失可能会造成数值上的不稳定，可以使用Gluon提供的softmax函数；

**3.4 定义优化方法**

通过Gluon的Trainer方法，传入定义的net的param参数，以及sgd的学习率进行优化，返回定义的变量trainer；

**3.5 训练模型**

训练方式照旧；



















































