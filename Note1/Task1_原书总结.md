# 线性回归

线性回归神经网络适合于输出为连续值得情景。

## 1. 损失函数

在模型训练中，需要衡量预测值和真实值之间得误差，那么通常是选取一个非负数作为误差，且数值越小表示误差越小。

当使得误差最小化的损失函数问题的解可以直接用公式来表达出来时，这类问题叫做解析解。如果不存在解析解的话，可以采用数值解的方法：通过优化算法有限次迭代模型参数来尽可能的降低损失函数的值。

**数值解优化算法**：小批量随机梯度下降->

1. 先随机选取一组模型参数的初始值，接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值；
2. 在每次迭代中，先随机均匀采样一个由固定树木训练数据样本所组成的小批量(mini-batch)，然后求小批量中数据样本的平均损失有关模型参数的导数(梯度)；
3. 用此梯度结果和预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。

超参数： 不是通过模型训练学出的，而是人为设定的参数(如：这里的批量大小和学习率)；

## 2. 线性回归神经网络

**全连接层**：输出层中的神经元和输入层中各个输入完全连接，这样的输出层叫做全连接层，或者稠密层。

### 手动实现

线性回归神经网络的基本元素包括：模型、训练数据、损失函数和优化算法。

**2.1 生成数据集**：生成数据集的过程中，除了xw+b生成的标签值外，再加上服从均值为0，标准差为0.01的正态分布。

**2.2 读取数据**：训练模型时，需要遍历数据集并不断读取小批量的数据样本；

**2.3 初始化模型参数**：将权重初始化为均值为0，标准差为0.01的符合正态分布的随机数，偏差b则初始化为0，并对创建的w和b这两个参数创建它们的梯度，目的是需要对这些参数求梯度来迭代参数的值；

**2.4 定义模型：**定义线性回归wx+b的向量化函数；

**2.5 定义损失函数：**使用平方损失来定义线性回归的损失函数，返回的结果要和y_hat的形式相同，真实的也要和y_hat的形式相同。

**2.6 定义优化算法：**定义自动求梯度的模块，这个模块是用来求一个批量样本的梯度和，用这个梯度和除以批量的大小得到这个param的平均值；

**2.7 训练模型：**在每次迭代求模型参数时，根据当前读取的一个batch_size的数据样本，通过调用反向传播函数backward来计算小批量随机梯度，并调用优化算法随机梯度下降，来迭代求出模型参数。

**(3.2 书中问题未解答)**

### 简洁实现

**2.1 生成数据集 ：**和手动实现道理一样；

**2.2 读取数据集：**：通过DataLoader的函数进行随机读取小批量；

**2.3 定义模型：**通过导入nn模块，先定义一个Sequential实例的模型变量net。Sequential实例可以看作是一个串联各个层的容器，在构造模型时，要在这个容器中依次添加层，处理输入的数据并传给下一层；

全连接层是一个Dense实例，通过nn.Dense来进行创建；

**2.4 初始化模型参数：**使用net前，需要通过init来初始化模型的参数，通过init.Normal的方法来生成符合正态分布的权重和零值的偏差；

**2.5 定义损失函数：**使用loss模块机械能损失函数的导入；

**2.7 定义优化方法：**创建一个Trainer实例，该实例会指定优化算法，并使用优化算法来迭代net实例中add嵌套的所有层以及参数；

**2.7 训练模型：**通过Trainer实例的step函数来迭代模型参数，通过对损失函数的backward()，来求梯度下降后的损失函数的值；

**(3.3 书中问题未解答)**



# Softmax回归

SoftMax回归适合于模型输出为离散值的情景。

## 1. 分类问题的损失函数

要选择使用更加适合离散值输出的模型来解决分类问题，考虑使用softmax回归模型。

**softmax回归模型：**和线性回归的相同点：都是将输入特征与权重做线性叠加；都是单层的神经网络，输出层是全连接层；和线性回归的不同点：softmax回归的输出值个数等于标签里的类别数。

**softmax运算：**为了得到离散值的输出，方法一是可以将值最大的输出所对应的类作为预测输出，

但是这样直接使用输出层的问题在于：输出值得范围不确定，很难来衡量离散值和不确定范围得输出值之间得误差；





